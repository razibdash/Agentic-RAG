{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a72ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from crewai import Agent\n",
    "from crewai import Crew\n",
    "from crewai import Task\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2394189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "942f870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model \n",
    "llm=ChatGroq(\n",
    "        temperature = 0.1,\n",
    "        model=\"llama3-70b-8192\",\n",
    "        api_key= os.getenv(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057fdf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "pdf_url = \"https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\"\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "with open(\"attenstion_is_all_you_need.pdf\", \"wb\") as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd3f3fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"attenstion_is_all_you_need.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e938395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6fb7673",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f8d6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_13684\\16761998.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
      "d:\\anaconda\\envs\\agentrag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n",
    "\n",
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddb559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d9c05e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a902ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_chain.invoke(\"What is the main contribution of the paper 'Attention Is All You Need'?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fcc5195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context does not explicitly mention the paper \"Attention Is All You Need\". However, based on the content, it appears to be describing a model that uses self-attention mechanisms, which is the main contribution of the paper \"Attention Is All You Need\" by Vaswani et al. (2017). The paper introduced the Transformer model, which relies solely on self-attention mechanisms to process input sequences, eliminating the need for traditional recurrent or convolutional neural networks.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c00b1b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "TAVILY_API_KEY = os.environ[\"TAVILY_API_KEY\"]\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "tavily_tool = TavilySearchResults(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0081f9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Attention Is All You Need\"( is a 2017 landmark( paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer \"Transformer (machine learning model)\"), based on the attention mechanism proposed in 2014 by Bahdanau et al.( It is considered a foundational( paper in modern artificial intelligence, and a main contributor to the AI boom, as the transformer approach has become the main architecture of a wide [...] In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.( This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and [...] The paper is most well known for the introduction of the Transformer architecture, which forms the underlying architecture for most forms of modern Large Language Models (LLMs). A key reason for why the architecture is preferred by most modern LLMs is the parallelizability of the architecture over its predecessors. This ensures that the operations necessary for training can be accelerated on a GPU allowing both faster training times and models of bigger sizes to be trained.\n"
     ]
    }
   ],
   "source": [
    "reuslt=tavily_tool.run(\"What is the main contribution of the paper 'Attention Is All You Need'?\"  )\n",
    "print(reuslt[0]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4208872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def router_tool(question):\n",
    "  \"\"\"Router Function\"\"\"\n",
    "  if 'self-attention' in question:\n",
    "    return 'vectorstore'\n",
    "  else:\n",
    "    return 'web_search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bde44d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creste router agent\n",
    "router_agent = Agent(\n",
    "     role='Router',\n",
    "    goal='Route user question to a vectorstore or web search',\n",
    "    backstory=(\n",
    "    \"You are an expert at routing a user question to a vectorstore or web search.\"\n",
    "    \"Use the vectorstore for questions on concept related to Retrieval-Augmented Generation.\"\n",
    "    \"You do not need to be stringent with the keywords in the question related to these topics. Otherwise, use web-search.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a10802b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creste Retriever Agent\n",
    "Retriever_Agent = Agent(\n",
    "role=\"Retriever\",\n",
    "goal=\"Use the information retrieved from the vectorstore to answer the question\",\n",
    "backstory=(\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the information present in the retrieved context to answer the question.\"\n",
    "    \"You have to provide a clear concise answer.\"\n",
    "),\n",
    "verbose=True,\n",
    "allow_delegation=False,\n",
    "llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6c2ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Grader_agent =  Agent(\n",
    "  role='Answer Grader',\n",
    "  goal='Filter out erroneous retrievals',\n",
    "  backstory=(\n",
    "    \"You are a grader assessing relevance of a retrieved document to a user question.\"\n",
    "    \"If the document contains keywords related to the user question, grade it as relevant.\"\n",
    "    \"It does not need to be a stringent test.You have to make sure that the answer is relevant to the question.\"\n",
    "  ),\n",
    "  verbose=True,\n",
    "  allow_delegation=False,\n",
    "  llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f9b625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hallucination_grader = Agent(\n",
    "    role=\"Hallucination Grader\",\n",
    "    goal=\"Filter out hallucination\",\n",
    "    backstory=(\n",
    "        \"You are a hallucination grader assessing whether an answer is grounded in / supported by a set of facts.\"\n",
    "        \"Make sure you meticulously review the answer and check if the response provided is in alignmnet with the question asked\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce8a60cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_grader = Agent(\n",
    "    role=\"Answer Grader\",\n",
    "    goal=\"Filter out hallucination from the answer.\",\n",
    "    backstory=(\n",
    "        \"You are a grader assessing whether an answer is useful to resolve a question.\"\n",
    "        \"Make sure you meticulously review the answer and check if it makes sense for the question asked\"\n",
    "        \"If the answer is relevant generate a clear and concise response.\"\n",
    "        \"If the answer gnerated is not relevant then perform a websearch using 'web_search_tool'\"\n",
    "    ),\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112f9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
